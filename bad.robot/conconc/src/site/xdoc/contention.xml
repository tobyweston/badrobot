<?xml version="1.0" encoding="UTF-8"?>

<!--
  ~ Copyright (c) 2009-2011, bad robot (london) ltd
  ~
  ~ Licensed under the Apache License, Version 2.0 (the "License");
  ~ you may not use this file except in compliance with the License.
  ~ You may obtain a copy of the License at
  ~
  ~      http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<!--
Fool Maven's validator:
(see http://jira.codehaus.org/browse/MSITE-440)
<document blah="true">
</document>
-->
<document xmlns="http://maven.apache.org/XDOC/2.0"
          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
          xsi:schemaLocation="http://maven.apache.org/XDOC/2.0 http://maven.apache.org/xsd/xdoc-2.0.xsd">

    <properties>
        <title>Contention Monitoring and Block/Wait Counts</title>
        <author>Toby Weston</author>
    </properties>

    <body>

        <section name="Contention Monitoring and Block/Wait Counts">

            <p>
                When making comparisons between the implementations, it may be useful to
                understand any contention between competing threads on resources. For us, we're interested in contention
                when accessing shared memory. When shared memory is protected (or serialised) by a single guard, several
                concurrent requests will <i>contend</i> for exclusive access to the guard. A <i>contention ratio</i> then,
                may be useful in understanding the potential strain a protected resources could come under.
            </p>
            <p>
                A simple contention ratio then can be seen as
            </p>
            <source>

    number of concurrent calls to a shared memory resource : number of guards

            </source>
            <p>
                For example, twenty concurrent calls to a single <code>synchronized</code> block would result in a ratio
                of
            </p>
            <source>

    20:1

            </source>
            <p>
                in other words, the single lock could service only 5% of the time.
            </p>
            <p>
                However, this is not an easy thing to predict or measure ahead of time. We're not going to be able to tell
                how many calls to a shared resource are going to be made at precisely the same time. We can however,
                measure the <i>affect</i> of some implied contention. For example, we can measure the number of failed
                guarded calls and compare these to the number of success calls under different loads. The implication
                here being that the throughput (the number of requests the system is going to be able to make) will be
                affected by the contention ratio and by extension the effectiveness of the guard implementation. A guard
                that processes multiple requests quickly will positively affect throughput compared to a guard that blocks
                excessively, which will slow things down. At least, that's the theory.
            </p>
            <p>
                However, because of the way in which our guards protect a shared resource, we can't just monitor
                the number of failed guarded calls and use it as an indicator of contention. For example, a Software
                Transaction Memory guard might retry dozens of time before failing, whereas a <code>Lock</code> based
                solution may fail on the first attempt and never retry. Both were asked just once to attempt the call,
                but the behaviour of the implementation dictates the number of retries. What number represents the
                request count, the original request (one) of the number of retries (dozens)?
            </p>
            <p>
                So, to paint a fuller picture, we need to understand our system in terms of contention (how likely is it
                that a service is going to be heavily contended) as well as how quickly the service can process the
                contention (both in terms of processing time and time spent co-ordinating access to shared memory) and
                the observable outcome to the throughput of the service. In summary, if shared memory isn't contended
                (under load), it's likely that co-ordinating efforts are having little affect on the throughput. We want
                to be able to stretch the system to simulate contention in order to observe the affect our guards have
                on throughput.
            </p>
            <p>
                Lets paraphrase some of these assumptions to make things clearer
            </p>
            <ul>
                <li>
                    <p>
                        we want to <b>exaggerate contention in order to exaggerate the affect of a <code>Guard</code> </b>
                        implementation (so that we can evaluate their use)
                    </p>
                </li>
                <li>
                    <p>
                        we can <b>increase the contention by increasing the load</b> (number of concurrent
                        requests) made to a service
                    </p>
                </li>
                <li>
                    <p>
                        <b>we can measure a block/wait count</b> for our <code>Guard</code> implementations
                    </p>
                </li>
                <li>
                    <p>
                        if we <b>tweak the load so that block/wait counts are similar, a measure of throughput indicates
                        effectiveness</b> of the <code>Guard</code> (as well as the intermediate code on the critical path)
                    </p>
                </li>
            </ul>

            <subsection name="The Blocking Coefficient">
                <p>
                    We introduced the idea of measuring blocked calls or waits above in order to better interpret
                    various measures to evaluate our<code>Guard</code>s. We can formalise the idea into talking about
                    a<i>blocking coefficient</i>. A coefficient is some multiplicative factor, a constant term affecting
                    a calculation. In our case, the blocking coefficient is amount of time not spent servicing a task
                    (blocking or waiting) expressed as a percentage (or fraction) of the total time.
                </p>
                <p>
                    The blocking coefficient as a fraction is a number between 0 and 1 to indicate how much a particular
                    task is blocking (or waiting). Zero indicates CPU intensive work (no blocking) and a number close to
                    one represents a heavily blocked task. A fully blocked task would have 1 as a coefficient.
                </p>
                <p>
                    For example, if a task is idling (waiting or blocked) 80% of the time and so actually processing 20%
                    of the time, the blocking coefficient is 0.8 (80 / 100).
                </p>
                <p>
                    In terms of our <code>Guard</code> implementations, we're able to monitor;
                </p>
                <ol>
                    <li>
                        <p>
                            <b>Blocking with Pessimistic (<code>synchronized</code>) Control </b> - the number of requests
                            (or time) blocked whilst attempting to acquire an object monitor.
                        </p>
                    </li>
                    <li>
                        <p>
                            <b>Waiting with Optimistic (<code>Lock</code>s) Control </b> - the number of requests (or
                            time) made to wait whilst attempting to acquire a lock.
                        </p>
                    </li>
                    <li>
                        <p>
                            <b>Contention with Optimistic (Software Transactional Memory) Control </b> - the number of
                            aborted atomic updates (this assumes an abort is the result of an attempt to access a
                            transactional reference which has already been accessed and not some other runtime exception).
                        </p>
                    </li>
                </ol>
                <p>
                    In all cases, we can use these values along with total request counts to give us our
                    coefficient. For example, the waiting or blocked count divided by the sample count where the sample
                    count is equal to the number of requests made. This gives the ratio of failed requests (due to blocked
                    or waiting). For example, if 10 calls out of 100 failed, the ratio would be 0.1 (10 / 100).
                </p>
                <p>
                    We can also use the waiting or blocked <i>time</i> divided by the CPU (service) time. This produces a similar indication as
                    above but this time in terms of time. For example, if thread A was busy for 1000 milliseconds and
                    waiting for 100 milliseconds, the result would be 0.1 milliseconds (100 microseconds).
                </p>
                <p>
                    See <a href="appendix_A.html">Appendix A</a> for some additional background around why and how
                    threads will enter blocked or waiting states.
                </p>
            </subsection>
            
            <subsection name="Blocking in Pessimistic Concurrency Control">
                <p>
                    To begin with we're interested in measuring the blocking coefficient caused by the waiting on monitor acquisition.
                    We can use the Java class <code>ThreadInfo</code> to get information about a particular thread
                    including the block count (the number of times the thread has been in the <code>BLOCKED</code> state)
                    and total elapsed time a thread has been blocked (again, the total time spent in the <code>BLOCKED</code>
                    state). The state transition to <code>BLOCKED</code> is only possible when a thread is waiting to
                    acquire (or re-acquire) an object's monitor.
                </p>
                <p>
                    Unfortunately, the <code>ThreadInfo</code> class doesn't distinguish between the
                    specific monitor a thread is blocked waiting to acquire but we can make some assuptions and provide
                    an approximation using the following code.
                </p>
                <source>
public class BlockingRatio {

    private final Counter count = new AtomicLongCounter();
    private final Map&lt;Long, Long&gt; blocked = new ConcurrentHashMap&lt;Long, Long&gt;();
    private final ThreadMXBean jvm;

    public BlockingRatio(Factory&lt;ThreadMXBean&gt; factory) {
        jvm = factory.create();
        if (jvm.isThreadContentionMonitoringSupported())
            jvm.setThreadContentionMonitoringEnabled(true);
    }

    public void sample() {
        count.increment();
        blocked.put(currentThread().getId(), getBlockedCount(currentThread()));
    }

    public Double get() {
        double ratio = 0;
        for (Long blocked : this.blocked.values())
            ratio += (double) blocked / (double) count.get();
        return ratio;
    }

    private long getBlockedCount(Thread thread) {
        return jvm.getThreadInfo(thread.getId()).getBlockedCount();
    }
}</source>
                <p>
                    Here, we assume that client will "sample" blocked calls at appropriate times which is basically during
                    load. We also brush over the fact that the instrumentation itself may influence the results. We use
                    an <code>AtomicLongCounter</code> and <code>ConcurrentHashMap</code> as they both offer optimistic
                    thread safety (with the implication being that they are fast). We also defer maintaining the
                    consistency of updating <code>count</code> and <code>blocked</code> together (for example, by using a
                    <code>Guard</code>) for the same reason; namely we're favouring performance over accuracy. We use a
                    factory to create the <code>ThreadMXBean</code> in order to be able to write unit style tests without
                    using real JVM thread metadata.
                </p>
                <p>
                    As we are interested in monitoring contention around locks, we can conveniently use the
                    <code>BlockingRatio</code> class from within a custom <code>Guard</code> implementation.
                    The guard is our abstraction for protecting resources and we're interested in understanding contention
                    at this point. As we're also interested in the total number of requests, we can employ the
                    <code>Throughput</code> class defined previously in the same place. For example,
                </p>
                <source>
public class ContentionMonitoringGuard implements Guard, ContentionMonitoringGuardMBean {

    private final BlockingRatio contention = new BlockingRatio(new JmxThreadMxBean());
    private final Throughput throughput;

    public ContentionMonitoringGuard(Throughput throughput) {
        this.throughput = throughput;
    }

    @Override
    public &lt;R, E extends Exception&gt; R execute(Callable&lt;R, E&gt; callable) throws E {
        synchronized (this) {
            RequestObserver.Request request = throughput.started();
            try {
                return callable.call();
            } finally {
                request.finished();
                <b>contention.sample();</b>
            }
        }
    }

    @Override
    public Boolean guarding() {
        return true;
    }

    @Override
    public Double getContentionRatio() {
        return contention.get();
    }

    @Override
    public Double getRequestsPerSecond() {
        return throughput.getRequestsPerSecond();
    }

    @Override
    public Long getTotalRequests() {
        return throughput.getTotalRequests();
    }
}                </source>

                <p>
                    We include the throughput as well as blocking coefficient so that we can adjust the load later.
                </p>
                <p>
                    An instance of this guard will be used to protect some shared resource and as such will sample the
                    current thread's block count just before releasing it's monitor. A thread which manages to execute the
                    guarded section (the call to <code>callable.call()</code>) without being blocked will record
                    no contention. If however, whilst that thread is executing the guarded section, another attempts to
                    do the same, it will block until the first has released the monitor. In this case, the second
                    thread will record a blocked attempt when executing the <code>contention.sample()</code> method. A
                    thread dump showing the kind of blocking behaviour that <code>ContentionMonitoringGuard</code> would
                    capture as contention is shown below.
                </p>
<source>
Thread Thread-0@9: (state = RUNNABLE)
 ...
 - bad.robot.pessimistic.ContentionMonitoringGuardTest$1.call(ContentionMonitoringGuardTest.java:14)
 - bad.robot.pessimistic.ContentionMonitoringGuard.execute(ContentionMonitoringGuard.java:36)
 - bad.robot.pessimistic.ContentionMonitoringGuardTest$3.run(ContentionMonitoringGuardTest.java:36)
 - java.lang.Thread.run(Thread.java:722)

Thread Thread-1@10: (state = BLOCKED)
 - bad.robot.pessimistic.ContentionMonitoringGuard.execute(ContentionMonitoringGuard.java:34)
 - bad.robot.pessimistic.ContentionMonitoringGuardTest$3.run(ContentionMonitoringGuardTest.java:36)
 - java.lang.Thread.run(Thread.java:722)
</source>

            <p>
                <code>Thread-0</code> acquired the guard's monitor and is executing (it's in the <code>RUNNABLE</code>
                state) whilst <code>Thread-1</code> is <code>BLOCKED</code> at the <code>execute</code> call.
                When <code>Thread-1</code> finally continues the <code>ContentionMonitoringGuard</code> would indicate a
                contention ratio of 0.5. Half the requests were contended.
            </p>
            </subsection>

            <subsection name="Waiting in Locks">
                <p>...</p>
            </subsection>

            <subsection name="Contention in Software Transaction Memory">
                <p>
                    Using hooks into the Multiverse STM library, we can observe the number of aborts vs the number of 
                    successful commits giving us the contention ratio. Multiverse allows us to add a <i>deferred task</i>
                    to execute on successful commit and a <i>compensating task</i> on aborts. Implementing basic tasks 
                    using our existing <code>Counter</code>s, we can wire up a basic contention monitoring <code>Guard</code>. 
                    For example, we can re-use the <code>Increment</code> class shown below to increment <code>Counter</code>s
                    <i>on abort</i> or <i>on commit</i> events.
                </p>
                <source>
public class Increment&lt;T extends Counter&gt; implements Callable&lt;Void, RuntimeException&gt; {

    private final Counter counter;

    public static &lt;T extends Counter&gt; Increment&lt;T&gt; increment(T counter) {
        return new Increment&lt;T&gt;(counter);
    }

    private Increment(T counter) {
        this.counter = counter;
    }

    @Override
    public Void call() throws RuntimeException {
        counter.increment();
        return null;
    }
}                    
                </source>
                <p>
                    The increment functionality is a <code>Callable</code>, so if we adapt it to either a <code>CompensatingTask</code>
                    or <code>DeferredTask</code>,
                </p>
                <source>
public final class CallableAdaptors {

    public static CompensatingTask onAbort(final Callable&lt;?, RuntimeException&gt; callable) {
        return new CompensatingTask() {
            @Override
            public void run() {
                callable.call();
            }
        };
    }

    public static DeferredTask onCommit(final Callable&lt;?, RuntimeException&gt; callable) {
        return new DeferredTask() {
            @Override
            public void run() {
                callable.call();
            }
        };
    }
}                    
                </source>
                <p>
                    we can then schedule increment behaviour on the events using our "runner" (and the infrastructure
                    supplied by Multiverse in the <code>STMUtils</code> class) below.
                </p>
                <source>
public class RunAtomically&lt;R, E extends Exception&gt; extends Atomic&lt;R&gt; {

    private final Callable&lt;R, E&gt; callable;
    private final DeferredTask onCommit;
    private final CompensatingTask onAbort;

    public static &lt;R, E extends Exception&gt; R runAtomically(Callable&lt;R, E&gt; callable) {
        return new RunAtomically&lt;R, E&gt;(callable, new DoNothingDeferredTask(), new DoNothingCompensatingTask()).execute();
    }

    public static &lt;R, E extends Exception&gt; R runAtomically(Callable&lt;R, E&gt; callable, DeferredTask onCommit, CompensatingTask onAbort) {
        return new RunAtomically&lt;R, E&gt;(callable, onCommit, onAbort).execute();
    }

    RunAtomically(Callable&lt;R, E&gt; callable, DeferredTask onCommit, CompensatingTask onAbort) {
        this.callable = callable;
        this.onCommit = onCommit;
        this.onAbort = onAbort;
    }

    @Override
    public R atomically() {
        try {
            <b>StmUtils.scheduleDeferredTask(onCommit);</b>
            <b>StmUtils.scheduleCompensatingTask(onAbort);</b>
            return callable.call();
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }
}                    
                </source>
                <p>
                    Finally, the <code>Guard</code> can use the counters to work out contention when using the "runner"
                    to <code>runAtomically</code>. For example,
                </p>
                <source>
public class ContentionMonitoringStmGuard implements Guard, ContentionMonitoringStmGuardMBean {

    private final Counter aborts = new AtomicLongCounter();
    private final Counter commits = new AtomicLongCounter();

    @Override
    public &lt;R, E extends Exception&gt; R execute(Callable&lt;R, E&gt; callable) throws E {
        return <b>runAtomically(callable, onCommit(increment(commits)), onAbort(increment(aborts)));</b>
    }

    @Override
    public Boolean guarding() {
        return true;
    }

    @Override
    public Double getContentionRatio() {
        return (double) aborts.get() / (double) commits.get();
    }
}                    
                </source>
                
            </subsection>

            <subsection name="Choosing the Optimal Number of Threads">
                <p>
                    As we saw earlier, if we can keep the blocking coefficient in a similar range by adjusting the load,
                    we can make stronger inferences against the resulting throughput. However, this isn't the whole
                    story. We also want to try and tune the system so that it's able to utilise the system processors
                    efficiently. It may therefore make sense to have a guide when deciding how many threads to use when
                    testing.
                </p>
                <h4>CPU Bound Tasks</h4>
                <p>
                    For CPU bound tasks, Goetz (2002, 2006.) recommends
                </p>
                <source>threads = number of CPUs + 1</source>
                <p>
                    Which is intuitive as if a CPU is being kept busy, we can't do more work than the number of CPUs.
                    Goetz purports that the additional CPU has been shown as an improvement over omitting it (2006. pp.XXX),
                    presumably helping with thread context switching.
                </p>
                <h4>IO Bound Tasks</h4>
                <p>
                    Working out the optimal number for IO bound tasks is less obvious. During an IO bound task, a CPU
                    will be left idle (waiting or blocking). This idle time can be better used in initiating another
                    IO bound request.
                </p>
                <p>
                    Subramaniam (2011, p.31) describes the optimal number of threads in terms of the following formula.
                </p>
                <source>threads = number of cores /  (1 – blocking coefficient)</source>
                <p>
                    <img alt="subramaniam" src="images/contention/subramaniam.gif"/>
                </p>

                <p>
                    And Goetz (2002) describes the optimal number of threads in terms of the following.
                </p>
                <source>threads = number of cores * (1 + wait time / service time)</source>
                <p>
                    <img alt="goetz" src="images/contention/goetz.gif" />
                </p>
                <p>
                    Where we can think of <code>wait time / service</code> time as a measure of how contended the task is.
                </p>
                <p>
                    When we use equivalent terms in Subramaniam‘s expression we can begin to form the proposition that
                    both formulas are equivalent. Starting with Goetz’s formula, we assert that w+s=1 and remove the
                    service time (s) from Goetz’s formula giving the following
                </p>
                <p>
                    <img alt="goetz" src="images/contention/goetz-2.gif"/>
                </p>

                <p>
                    We can continue by multiplying both sides by 1-w reducing the right hand side to c before reversing
                    the operation and revealing Subramaniam’s expression.
                </p>
                <p>
                    <img alt="goetz" src="images/contention/goetz-3.gif" />
                </p>
                <p>
                    <img alt="goetz" src="images/contention/goetz-4.gif" />
                </p>
                <p>
                    <img alt="subramaniam" src="images/contention/subramaniam.gif"/>
                </p>

                <p>
                    As we were able to show that Subramaniam and Goetz agree on the number of threads to use for IO
                    bound tasks, we'll be confident in our choices of thread pool sizes when it comes to performance
                    testing later.
                </p>
            </subsection>

        </section>

    </body>

</document>